{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English-to-Arabic Translation Model\n",
    "\n",
    "This project implements a sequence-to-sequence neural network model with attention for translating English sentences to Arabic. The model is built using TensorFlow and Keras, leveraging LSTM layers and a custom attention mechanism for improved translation quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T21:30:49.078254Z",
     "iopub.status.busy": "2024-11-06T21:30:49.077737Z",
     "iopub.status.idle": "2024-11-06T21:31:02.315186Z",
     "shell.execute_reply": "2024-11-06T21:31:02.3136Z",
     "shell.execute_reply.started": "2024-11-06T21:30:49.078204Z"
    },
    "id": "HbD_bU5FgFXO",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input, TimeDistributed, Bidirectional, Concatenate\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T21:31:02.318441Z",
     "iopub.status.busy": "2024-11-06T21:31:02.317718Z",
     "iopub.status.idle": "2024-11-06T21:31:02.556358Z",
     "shell.execute_reply": "2024-11-06T21:31:02.554982Z",
     "shell.execute_reply.started": "2024-11-06T21:31:02.318394Z"
    },
    "id": "_HECyN02gFXO",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/kaggle/input/arabic-to-english-translation-sentences/ara_eng.txt', encoding='utf-8', sep='\\t', names=['English', 'Arabic'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T21:31:02.558464Z",
     "iopub.status.busy": "2024-11-06T21:31:02.558052Z",
     "iopub.status.idle": "2024-11-06T21:31:02.585675Z",
     "shell.execute_reply": "2024-11-06T21:31:02.584509Z",
     "shell.execute_reply.started": "2024-11-06T21:31:02.558422Z"
    },
    "id": "J-7siBzfgFXP",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T21:31:02.589371Z",
     "iopub.status.busy": "2024-11-06T21:31:02.588884Z",
     "iopub.status.idle": "2024-11-06T21:31:02.613531Z",
     "shell.execute_reply": "2024-11-06T21:31:02.612286Z",
     "shell.execute_reply.started": "2024-11-06T21:31:02.589326Z"
    },
    "id": "v6dHiBRsgFXP",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "input_texts = df['English'].values\n",
    "\n",
    "target_texts = df['Arabic'].values\n",
    "\n",
    "\n",
    "\n",
    "target_texts = [\"<start> \" + text + \" <end>\" for text in target_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T21:31:02.615514Z",
     "iopub.status.busy": "2024-11-06T21:31:02.615095Z",
     "iopub.status.idle": "2024-11-06T21:31:05.990526Z",
     "shell.execute_reply": "2024-11-06T21:31:05.989398Z",
     "shell.execute_reply.started": "2024-11-06T21:31:02.615472Z"
    },
    "id": "N_AY8qXngFXQ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer_in = Tokenizer()\n",
    "\n",
    "tokenizer_in.fit_on_texts(input_texts)\n",
    "\n",
    "input_sequences = tokenizer_in.texts_to_sequences(input_texts)\n",
    "\n",
    "input_sequences = pad_sequences(input_sequences, padding='post')\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize the target sequences (Arabic)\n",
    "\n",
    "tokenizer_out = Tokenizer(filters='')  # Disable filters to keep '<start>' and '<end>' tokens\n",
    "\n",
    "tokenizer_out.fit_on_texts(target_texts)\n",
    "\n",
    "target_sequences = tokenizer_out.texts_to_sequences(target_texts)\n",
    "\n",
    "target_sequences = pad_sequences(target_sequences, padding='post')\n",
    "\n",
    "\n",
    "\n",
    "# Get vocabulary sizes\n",
    "\n",
    "num_encoder_tokens = len(tokenizer_in.word_index) + 1\n",
    "\n",
    "num_decoder_tokens = len(tokenizer_out.word_index) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T21:31:05.992393Z",
     "iopub.status.busy": "2024-11-06T21:31:05.992007Z",
     "iopub.status.idle": "2024-11-06T21:31:06.041585Z",
     "shell.execute_reply": "2024-11-06T21:31:06.040187Z",
     "shell.execute_reply.started": "2024-11-06T21:31:05.992353Z"
    },
    "id": "QI1WmkCmgFXQ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Get max sequence lengths\n",
    "\n",
    "max_encoder_seq_length = max([len(seq) for seq in input_sequences])\n",
    "\n",
    "max_decoder_seq_length = max([len(seq) for seq in target_sequences])\n",
    "\n",
    "\n",
    "\n",
    "# Prepare the target data for the decoder (shifted by one for teacher forcing)\n",
    "\n",
    "decoder_input_sequences = np.zeros_like(target_sequences)\n",
    "\n",
    "decoder_input_sequences[:, 1:] = target_sequences[:, :-1]\n",
    "\n",
    "decoder_input_sequences[:, 0] = tokenizer_out.word_index['<start>']\n",
    "\n",
    "\n",
    "\n",
    "# Prepare the target data (shifted by one for the output)\n",
    "\n",
    "decoder_target_sequences = np.zeros_like(target_sequences)\n",
    "\n",
    "decoder_target_sequences[:, :-1] = target_sequences[:, 1:]\n",
    "\n",
    "decoder_target_sequences[:, -1] = 0\n",
    "\n",
    "\n",
    "\n",
    "# Expand dimensions of target data for sparse categorical crossentropy\n",
    "\n",
    "decoder_target_sequences = np.expand_dims(decoder_target_sequences, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T21:31:06.044236Z",
     "iopub.status.busy": "2024-11-06T21:31:06.043637Z",
     "iopub.status.idle": "2024-11-06T21:31:09.104205Z",
     "shell.execute_reply": "2024-11-06T21:31:09.102792Z",
     "shell.execute_reply.started": "2024-11-06T21:31:06.044174Z"
    },
    "id": "mwiEw7iEgFXQ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "EMBEDDING_DIM = 256\n",
    "\n",
    "HIDDEN_UNITS = 512\n",
    "\n",
    "\n",
    "\n",
    "# Define the encoder\n",
    "\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "\n",
    "encoder_embedding = Embedding(num_encoder_tokens, EMBEDDING_DIM)(encoder_inputs)\n",
    "\n",
    "encoder_lstm = Bidirectional(LSTM(HIDDEN_UNITS, return_state=True, return_sequences=True))\n",
    "\n",
    "encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder_lstm(encoder_embedding)\n",
    "\n",
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "\n",
    "state_c = Concatenate()([forward_c, backward_c])\n",
    "\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "\n",
    "\n",
    "# Define the decoder\n",
    "\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "decoder_embedding = Embedding(num_decoder_tokens, EMBEDDING_DIM)(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(HIDDEN_UNITS * 2, return_sequences=True, return_state=True)\n",
    "\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "\n",
    "\n",
    "\n",
    "# Attention mechanism\n",
    "\n",
    "attention = tf.keras.layers.Attention()\n",
    "\n",
    "attention_output = attention([decoder_outputs, encoder_outputs])\n",
    "\n",
    "\n",
    "\n",
    "# Concatenate attention output and decoder LSTM output\n",
    "\n",
    "decoder_concat_input = Concatenate(axis=-1)([decoder_outputs, attention_output])\n",
    "\n",
    "\n",
    "\n",
    "# Dense layer to generate predicted words\n",
    "\n",
    "decoder_dense = TimeDistributed(Dense(num_decoder_tokens, activation='softmax'))\n",
    "\n",
    "decoder_outputs = decoder_dense(decoder_concat_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T21:31:09.106405Z",
     "iopub.status.busy": "2024-11-06T21:31:09.105892Z"
    },
    "id": "-2S_8b_ZgFXR",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "\n",
    "\n",
    "# Train the model\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "\n",
    "\n",
    "model.fit([input_sequences, decoder_input_sequences], decoder_target_sequences, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=0.2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pso577ufgFXS",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, [encoder_outputs, state_h, state_c])\n",
    "\n",
    "\n",
    "\n",
    "# Decoder model\n",
    "\n",
    "decoder_state_input_h = Input(shape=(HIDDEN_UNITS * 2,))\n",
    "\n",
    "decoder_state_input_c = Input(shape=(HIDDEN_UNITS * 2,))\n",
    "\n",
    "decoder_hidden_state_input = Input(shape=(max_encoder_seq_length, HIDDEN_UNITS * 2))\n",
    "\n",
    "\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "\n",
    "    decoder_embedding, initial_state=[decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "attention_output = attention([decoder_outputs, decoder_hidden_state_input])\n",
    "\n",
    "decoder_concat_input = Concatenate(axis=-1)([decoder_outputs, attention_output])\n",
    "\n",
    "decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "\n",
    "\n",
    "decoder_model = Model(\n",
    "\n",
    "    [decoder_inputs] + [decoder_hidden_state_input, decoder_state_input_h, decoder_state_input_c],\n",
    "\n",
    "    [decoder_outputs] + [state_h, state_c]\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tPGBINmTgFXT",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Translation function\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "\n",
    "    # Encode the input as state vectors.\n",
    "\n",
    "    enc_out, h, c = encoder_model.predict(input_seq)\n",
    "\n",
    "\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "\n",
    "    target_seq = np.zeros((1, 1))\n",
    "\n",
    "    target_seq[0, 0] = tokenizer_out.word_index['<start>']\n",
    "\n",
    "\n",
    "\n",
    "    stop_condition = False\n",
    "\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "\n",
    "\n",
    "    while not stop_condition:\n",
    "\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq, enc_out, h, c])\n",
    "\n",
    "\n",
    "\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "\n",
    "        sampled_word = tokenizer_out.index_word[sampled_token_index]\n",
    "\n",
    "        decoded_sentence += \" \" + sampled_word\n",
    "\n",
    "\n",
    "\n",
    "        if sampled_word == '<end>' or len(decoded_sentence.split()) > max_decoder_seq_length:\n",
    "\n",
    "            stop_condition = True\n",
    "\n",
    "\n",
    "\n",
    "        target_seq = np.zeros((1, 1))\n",
    "\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "\n",
    "\n",
    "    return decoded_sentence.strip('<start> ').strip(' <end>')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FkRs2ZNIgFXT",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def translate(input_text):\n",
    "\n",
    "    # Tokenize the input text\n",
    "\n",
    "    input_sequence = tokenizer_in.texts_to_sequences([input_text])\n",
    "\n",
    "    input_sequence = pad_sequences(input_sequence, maxlen=max_encoder_seq_length, padding='post')\n",
    "\n",
    "\n",
    "\n",
    "    # Perform translation\n",
    "\n",
    "    decoded_sentence = decode_sequence(input_sequence)\n",
    "\n",
    "\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "input_text = \"I'm sorry\"\n",
    "\n",
    "translated_sentence = translate(input_text)\n",
    "\n",
    "print(\"Translated sentence:\", translated_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iRhM0-xdgFXU",
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m0UYJQcmgFXU",
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "english translate to arabic",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 492069,
     "sourceId": 915247,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
